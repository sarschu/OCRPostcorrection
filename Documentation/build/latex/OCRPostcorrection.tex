%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
 \ifdefined\DeclareUnicodeCharacterAsOptional
  \DeclareUnicodeCharacter{"00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{"2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{"2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{"2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{"251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{"2572}{\textbackslash}
 \else
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{2572}{\textbackslash}
 \fi
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage[dontkeepoldnames]{sphinx}

\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\addto\captionsenglish{\renewcommand{\tablename}{Table}}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{1}



\title{OCRPostcorrection Documentation}
\date{Sep 18, 2017}
\release{1}
\author{Sarah Schulz}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex

\begin{document}

\maketitle
\sphinxtableofcontents
\phantomsection\label{\detokenize{index::doc}}


Contents:


\chapter{Tutorial}
\label{\detokenize{README:welcome-to-nougat-s-documentation}}\label{\detokenize{README:tutorial}}\label{\detokenize{README::doc}}
Contributors: Sarah Schulz
Based upon a System for Normalization of User-Generated Text developed in collaboration with Bart Desmet and Orphee DeClercq
Version 1.0

Date September 2017

contact: \sphinxhref{mailto:sarah.schulz@ims.uni-stuttgart.de}{sarah.schulz@ims.uni-stuttgart.de}

The system reads plain text that has been OCRed and returns a version in which errors have been corrected.
For information on the training data etc. check out the EMNLP paper at \sphinxurl{http://aclanthology.info/papers/D17-1287/d17-1287}

The repository does not contain all models due to size limits. If you want to use them, please, send me a mail.


\section{System architecture}
\label{\detokenize{README:system-architecture}}
The pipeline consists of different more or less independent modules.
\begin{itemize}
\item {} 
the preprocessing module

\item {} 
the suggestion-generation modules

\item {} 
the decision module

\end{itemize}


\subsection{Preprocessing}
\label{\detokenize{README:preprocessing}}
The first step that is performed is the preprocessing of the input files.

The program works on the document line level. The tokenization is done with the TreeTagger tokenizer.


\subsection{Suggestion generation}
\label{\detokenize{README:suggestion-generation}}
There are different modules that generate possible correction option for each word in a line. Some modules
generate exactly one output for each word, others do not deliver an option for every word but can deliver more than one
possible option per word.

The modules cover different levels of recognition errors that can appear OCRed text.


\subsubsection{Compound}
\label{\detokenize{README:compound}}
It is often the case that spaces are introduced where there should not be any.

To account for this compounding mistakes the compound module tests for all two words that are written next to each other if a
spell checker (hunspell) recognizes them as correctly spelled word when they are written as one word. If that is the case, the
compound version is returned as a possible option for the two words.

The output of this module is a phrase (the two original words) along with a one word option. This is possible since we later one work
with phrase-based machine translation. If there is no compound correction in the input sentence the output is an empty list.


\subsubsection{Original}
\label{\detokenize{README:original}}
The original module returns the original word as an option for the word. This is important since some of the words are not
erroneous and could get lost when no other module returns the original option.

The output of this module is the original word along with exactly one option (which is the same word as the input word).


\subsubsection{SMT}
\label{\detokenize{README:smt}}
Following preliminary experiments described in DeClercq (2013), the SMT models have been trained on token and character level using Moses. The language model used has been built from a selection of German Gutenberg texts using KenLM.

There are four different modules that work with statistical machine translation on character and token level.

All those modules work with Moses models. Since the program works on the sentence level we included Moses sever mode in order to avoid loading the model files of Moses for each sentence.


\subsubsection{Spell checker}
\label{\detokenize{README:spell-checker}}
The spell checker module uses hunspell (and its python wrapper pyhunspell). Each word in the original sentence is spell checked.
In case hunspell classifies a word as wrongly-spelled, the correction suggestions given by hunspell are returned as alternatives.
The output of the spell checker module is a list of spell checker suggestions for each wrongly-spelled word.


\subsubsection{Word split}
\label{\detokenize{README:word-split}}
Theword split module is the opposite of the compound module and splits words that have been erroneously written together.

The word split module is based on the compound-splitter module of Moses  and has been trained on the Gutenberg corpus.
It often appears that words that are actually two words are written together.


\subsubsection{Specific vocab}
\label{\detokenize{README:specific-vocab}}
Extracts words that are frequent from the erroneous text as a basis for word to word comparison via Levenshtein. Makes e.g. the correction of Named Entities possible.


\subsection{Decision module}
\label{\detokenize{README:decision-module}}
Since we have no a-priori knowledge about the nature of a normalization problem, each sentence is sent to all modules of the suggestion  layer. In order to prevent an avalanche of suggestions, for each module of the suggestion layer, we restricted the number of suggestions per token to one.

It is the task of the decision module to choose the most probable combination of suggestions  to build a well-formed sentence, which poses a combinatorial problem. The decision module itself makes use of the Moses decoder, a powerful, highly efficient tool able to solve the combinatorial problem with the aid of a language model of standard languagein order to include probability information.

We include the correction suggestions in form of a phrase table into the decoding process. The decoder weights have been manually tuned using development data. However, the weights might not be ideal for decoding the dynamically compiled phrase tables containing the normalization suggestions. We include weights for the different components of the decoding process like the language model. the translation model, word penalty and distortion. Distortion is made expensive since we want to avoid reordering. Moreover, the decoder uses features containing information about which module returned which suggestion. The feature weights are first set to 0.2 and later tuned on the development data


\subsubsection{LOGFILES:}
\label{\detokenize{README:logfiles}}
Every evaluation run, will create a variety of files:

log files: those files you can find in ../log/runs and will have the starting date and time as a name

They log the output list for every module for every sentence and several other stuff.


\section{Example of usage}
\label{\detokenize{README:example-of-usage}}
BEFORE GETTING STARTED:

check if there is a server mode running without you knowing it. In general, the system should
kill them in the end of a run or in case of an error, but I don’t know why, sometimes that fails.

ps aux \textbar{} grep moses

\textendash{}\textgreater{} kill -9 pids

The system requires 3 arguments:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{run\PYGZus{}norm}\PYG{o}{.}\PYG{n}{py} \PYG{o}{\PYGZlt{}}\PYG{n}{ini\PYGZus{}file}\PYG{o}{\PYGZgt{}} \PYG{o}{\PYGZlt{}}\PYG{n}{inputfile}\PYG{o}{\PYGZgt{}} \PYG{o}{\PYGZlt{}}\PYG{n}{outputfile}\PYG{o}{\PYGZgt{}}
\end{sphinxVerbatim}


\subsection{Ini file}
\label{\detokenize{README:ini-file}}
The ini files is the specification file of the system. You have to adjust it to your settings before you run normalization. Make sure you use absolute pathes (if you use relative pathes they have to be relative to your norm directory). There are two default ini files included in the system. In case you want to run default settings, you can leave them as they are. You find them in norm/ini\_files. The following options have to be specified:
\begin{itemize}
\item {} 
{[}MOSES-PATH{]} define where Moses is installed on your machine
\begin{itemize}
\item {} 
Moses: where is Moses itself installed

\item {} 
Mosesserver: where is Mosesserver installed

\end{itemize}

\item {} 
{[}SMT{]} define where the ini files for the different MT systems are located
\begin{itemize}
\item {} 
Token:  define where the ini file the MT token model is located

\item {} 
Unigram: define where the ini file the MT token model is located

\item {} 
Bigram: define where the ini file the MT token model is located

\item {} 
Decision: define where the ini file the MT token model is located

\end{itemize}

\item {} 
{[}SETTING{]} define if you use unbalanced or balanced file or just SMS, TWE, SNS …
\begin{itemize}
\item {} 
train\_set: sets the setting for the transliterate module

\end{itemize}

\item {} 
{[}LM{]} define where to find the lm used in the decision module
\begin{itemize}
\item {} 
lm

\end{itemize}

\item {} 
{[}Modules{]} define which modules should be used, this has to match your decision ini file (number of weights)
\begin{itemize}
\item {} 
mod: give the modules separated by white space, the order should be the same as the order you had while compiling the tuning table on the development set. You can choose out of the following:  Original SMT\_Token SMT\_Token2 SMT\_Unigram SMT\_Unigram2 SMT\_Cascaded Hunspell LM Compound Word\_Split

\end{itemize}

\end{itemize}


\chapter{Development}
\label{\detokenize{README:development}}
You might bump into situations in which you want to use a new setting for which there does’nt exist a decison.ini file, yet. This situations can be:
\begin{itemize}
\item {} 
you use a different combination of modules

\end{itemize}

What do you have to do then:
\begin{enumerate}
\item {} 
fix your ini file
\begin{itemize}
\item {} 
SMT inis in the right setting

\item {} 
same setting for train\_set

\item {} 
put develoment switch dev to True (make sure you dont have anything in ../log/phrasetables that you might need later)

\item {} 
check if you have the right modules involved (the order of the modules that you define here, will be the order you have to keep whenever you use this setting)

\item {} 
put the right filtering method

\item {} 
in the decision file, make sure you have the same number of feature weights in it as number of modules + 1 (hard, none filtering) +3 (soft filtering)

\end{itemize}

\item {} 
run the system on the development file without evaluation setting:
\begin{quote}

run\_norm.py \textless{}ini\_file\textgreater{} \textless{}input\_file\textgreater{} \textless{}output\_file\textgreater{}
\end{quote}

\item {} 
onces you have all phrase tables, combine them to one phrase table:

cat ./log/phrasetables/* \textgreater{} all\_phrasetables

\item {} 
make a new folder which you want to use for tuning (I used /home/sarah/Normalization/tuning)

\item {} 
copy the decision\_untuned.ini from one of my folders there and make sure that you include the right number of weights (number of modules +1 or +3 (soft filtering))

\item {} 
adjust the pathes in the following command and start tuning by doing so:
\begin{quote}

/opt/moses/moses\_with\_kenlm\_10\_compact/mosesdecoder-RELEASE-2.1.1/scripts/training/mert-moses.pl /home/sarah/Normalization/tuning/tune\_files/all\_dev\_clean.ori.prepro /home/sarah/Normalization/tuning/tune\_files/all\_dev\_clean.tgt.prepro /opt/moses/moses\_with\_kenlm\_10\_compact/mosesdecoder-RELEASE-2.1.1/bin/moses decision\_untuned.ini \textendash{}mertdir /opt/moses/moses\_with\_kenlm\_10\_compact/mosesdecoder-RELEASE-2.1.1/bin/ \&\textgreater{} mert.out
\end{quote}

\item {} 
wait

\item {} 
you find the new decision.ini in the mert folder. It is called moses.ini. They are on the bottom and look like that:
\begin{quote}

KENLM0= 0.000984536
Distortion0= 0.00987259
WordPenalty0= -0.946984
PhraseDictionaryMemory0= 0.00269802 0.00264633 0.00203958 0.00304969 0.00890275 0.00344056 0.0112828 -0.00027502 -0.00314784 0.00212978 0.00254623
\end{quote}

\item {} 
open one of the already existing ini file in the static/Moses/decoder\_files directory and put your weights there. This way you make sure that the other pathes are still correct; you should copy the feature weights in one of the old decision ini files to keep the old format. the new format doesn’t work with it.
\begin{itemize}
\item {} 
KENLM0 is {[}weight-l{]}

\item {} 
Distortion0 is {[}weight-d{]}

\item {} 
WordPenalty0 is {[}weight-w{]}

\item {} 
PhraseDictionaryMemory0 is {[}weight-t{]}

\end{itemize}

\end{enumerate}


\section{Trouble shooting of knowing where trouble often emerges}
\label{\detokenize{README:trouble-shooting-of-knowing-where-trouble-often-emerges}}\begin{itemize}
\item {} 
although the system worked the last 1000 times there is no garantee it will work the next time! DON’T ASK ME WHY

\item {} 
still, some stuff I might have seen before. So, if you can’t find the solution after some search, mail me

\item {} 
check if all you input files and input pathes do really exist (and by that I mean: verify with bash)

\item {} 
if you get an error in the decision module it is highly probable that one of the files like LM or phrase table does not exist, or the number of weights is incorrect

\item {} 
never use old and new ini file format together. The decision module works with the old format (if you ever want to change that, you will have to change the command line

\end{itemize}

arguments in the decision module subprocess call to the new commandline way of reading in phrase tables and language models)
* if one of the SMT modules crashes, there might be problems with the server modes. Check in the log files if they started correctly
* while tuning you might run into a problem with the filtering method of Moses. I don’t really know a direct solution for that. But google it. You are not alone
* should you ever run on thoth: be careful, the Moses path is different here
* be sure that the tuning order of the modules is the same as the running order (otherwise the weights are wrong)
* in case a key cannot be found during evaluation, make sure there are no weird characters like u”u200e” envolved in csv or ori file
* in case you use evaluation and there is a problem with one specific sentence although the others ran through: check the csv file. There might be something weird in the
annotation. Btw: the run\_norm.py script checks now if all sentences in input can be found as key in dict before starting. If not, it will throw and exception.
* I know that the alignment method is error prone, so be aware that mistakes can come up there


\chapter{System Requirements}
\label{\detokenize{README:system-requirements}}\label{\detokenize{README:sys-re}}
The system has been tested on a 64-bit Ubuntu machine.
The following tools and packages have to be installed and the paths have to be adjusted in order to run the normalizer.
\begin{itemize}
\item {} 
Moses (compiled with server mode and SRILM setting)

\item {} 
xmlrpc-c 1.25.23 (Moses has to be compiled with the \textendash{}with-xmlrpc-c in order to include it)

\item {} 
hunspell

\item {} 
pyhunspell 0.1

\item {} 
pytest

\item {} 
scikit-learn

\item {} 
nltk

\item {} 
kenlm

\end{itemize}


\chapter{API}
\label{\detokenize{API:api}}\label{\detokenize{API::doc}}

\section{Preprocessing}
\label{\detokenize{API:preprocessing}}

\section{Main}
\label{\detokenize{API:main}}

\section{Modules}
\label{\detokenize{API:modules}}\phantomsection\label{\detokenize{API:module-norm.modules.spellcheck}}\index{norm.modules.spellcheck (module)}\index{Hunspell (class in norm.modules.spellcheck)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{API:norm.modules.spellcheck.Hunspell}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{norm.modules.spellcheck.}\sphinxbfcode{Hunspell}}{\emph{normalizer}}{}
This class contains functions that check a word with the hunspell spell checker.
In case it is recognized as incorrectly spelled, alternative spelling options are suggested
\index{find\_suggestions() (norm.modules.spellcheck.Hunspell method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{API:norm.modules.spellcheck.Hunspell.find_suggestions}}\pysiglinewithargsret{\sphinxbfcode{find\_suggestions}}{\emph{word}}{}
use the hunspell spell checker for correct suggestions. take the first suggestion (levenshtein distance smallest).
\begin{description}
\item[{\sphinxstylestrong{parameters}, \sphinxstylestrong{types},**return**,**return types**::}] \leavevmode\begin{quote}\begin{description}
\item[{param word}] \leavevmode
a token

\item[{type sentence}] \leavevmode
unicode string

\item[{return}] \leavevmode
hunspell corrected suggestion

\item[{rtype}] \leavevmode
unicode string

\end{description}\end{quote}

\end{description}

\end{fulllineitems}

\index{generate\_alternatives() (norm.modules.spellcheck.Hunspell method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{API:norm.modules.spellcheck.Hunspell.generate_alternatives}}\pysiglinewithargsret{\sphinxbfcode{generate\_alternatives}}{\emph{sentence}, \emph{corr\_list}}{}
Generate suggestion
\begin{description}
\item[{\sphinxstylestrong{parameters}, \sphinxstylestrong{types},**return**,**return types**::}] \leavevmode\begin{quote}\begin{description}
\item[{param sentence}] \leavevmode
flooding corrected original message

\item[{type sentence}] \leavevmode
unicode string

\item[{param corr\_list}] \leavevmode
list containing all indices of the tokens that have to be normalized (hard filtering just some, otherwise all)

\item[{type corr\_list}] \leavevmode
list of integers

\item[{return}] \leavevmode
original tokens aligned with the suggestion of the form {[}{[}ori,{[}sug{]}{]},{[}ori2,{[}sug2{]}{]}{]}

\item[{rtype}] \leavevmode
list of lists

\end{description}\end{quote}

\end{description}

\end{fulllineitems}


\end{fulllineitems}



\section{Evaluation}
\label{\detokenize{API:evaluation}}

\section{Util}
\label{\detokenize{API:util}}

\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{n}
\item {\sphinxstyleindexentry{norm.modules.spellcheck}}\sphinxstyleindexpageref{API:\detokenize{module-norm.modules.spellcheck}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}